{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src=\"imagenes/cabecera.png\" width=\"900\" align=\"center\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo práctico 2: Modelos de $n$-gramas\n",
    "\n",
    "## Curso Procesamiento de Lenguaje Natural \n",
    "\n",
    "### Maestría en Tecnologías de la información\n",
    "\n",
    "\n",
    "\n",
    "**Trabajo práctico porpuesto por:** Julio Waissman Vilanova (julio.waissman@unison.mx)\n",
    "\n",
    "**Desarrollado por:** _Poner tu nombre y correo electrónico aquí_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este trabajo práctico tiene como objetivo la generalización de los modelos probabilisticos de trigramas que se desarrollaron en el curso a $n$-gramas, con $n \\ge 3$. Para modelos de mayor tamaño es necesario contar con textos de mayor amplitud, ya que si nos quedamos con un corpus pequeño, será difícil encontrar 5-gramas, por ejemplo, que se repitan suficientemente para tener estadísticas suficientes para un modelo. Es por esta razón que vamos a utilizar como corpus el libro de *El ingenioso Hidalgo, Don Quijote de la Mancha*.\n",
    "\n",
    "## 1. Normalización del corpus\n",
    "\n",
    "Para este corpus vamos a separar el corpus en párrafos (y no en frases como en el ejemplo de las libretas), y los párrafos en tokens, para entrenar por frases. Vamos a considerar que un párrafo se distingue de otro parque hay al menos un linea en blanco entre ambos.\n",
    "\n",
    "Una vez separado el texto en párrafos, es necesario normalizar la información. Vamos a probar tres variantes:\n",
    "\n",
    "1. Dejar el texto como está y separar las palabras de los signos de ortografía como tokens diferentes, esperando que el modelo sea capaz de representar correctamente el uso de símbolos ortográficos y el uso de mayúsculas y minúsculas.\n",
    "\n",
    "2. Eliminar los signos ortográficos, y usar todo el texto en minúsculas (y por lo tanto separa por palabras).\n",
    "\n",
    "3. Eliminar los signos ortográficos, usar todo el texto en minúsculas, eliminar las palabras de paro, y aplicar un método de *stemming*. El texto no será tan legible pero suponemos que generalizara mejor (vamos a ver).\n",
    "\n",
    "** Completa las dos funciones de tokenización de párrafo a tokens individuales que faltan.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "import nltk\n",
    "import collections\n",
    "\n",
    "\n",
    "\n",
    "def texto_a_parrafo(texto):\n",
    "    \"\"\"\n",
    "    Generador que separa un texto en párrafos, donde un párrafo\n",
    "    se distingue de otro por tener al menos una linea en blanco entre ellos.\n",
    "    \n",
    "    :param texto: Cadena de caracteres con e texto en cuestion\n",
    "    \n",
    "    :yield párrfo por párrafo de texto\n",
    "    \n",
    "    \"\"\"\n",
    "    parrafo = ''\n",
    "    for linea in texto.split('\\n'):   # Por cada linea de el texto\n",
    "        parrafo += linea\n",
    "        if len(linea) == 0 and parrafo is not '':\n",
    "            yield parrafo\n",
    "            parrafo = ''\n",
    "\n",
    "def parrafo_a_tokens_1(parrafo):\n",
    "    \"\"\"\n",
    "    Generador que separa un texto en tokens, los cuales se separan\n",
    "    por palabras diferentes y tambien por puntuación. El texto no se trata\n",
    "    y se envía con todo y simbolos, mayúsculas y minusculas.\n",
    "    \n",
    "    :param texto: Cadena de caracteres de un párrafo\n",
    "    \n",
    "    :yield token por token\n",
    "    \n",
    "    \"\"\"\n",
    "    return nltk.tokenize.wordpunct_tokenize(parrafo)\n",
    "\n",
    "def parrafo_a_tokens_2(parrafo):\n",
    "    \"\"\"\n",
    "    Generador que separa un texto en tokens. El texto primero se\n",
    "    normaliza en minúsculas y se eliminan los signos de puntuación.\n",
    "    Cada token es una palabra diferente\n",
    "    \n",
    "    :param texto: Cadena de caracteres de un párrafo\n",
    "    \n",
    "    :yield token por token\n",
    "    \n",
    "    \"\"\"\n",
    "    # AGREGAR AQUI TU CÖDIGO\n",
    "    raise NotImplementedError(\"Falta implementar la función\")\n",
    "    \n",
    "def parrafo_a_tokens_3(parrafo):\n",
    "    \"\"\"\n",
    "    Generador que separa un texto en tokens. El texto primero se\n",
    "    normaliza en minúsculas y se eliminan los signos de puntuación.\n",
    "    Adicionalmente, se eliminan las palabras de paro y se aplica\n",
    "    un proceso de stemming.\n",
    "    \n",
    "    :param texto: Cadena de caracteres de un párrafo\n",
    "    \n",
    "    :yield token por token\n",
    "    \n",
    "    \"\"\"\n",
    "    # AGREGAR AQUI TU CÖDIGO\n",
    "    raise NotImplementedError(\"Falta implementar la función\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a probar nuestras funciones con el texto seleccionado. Revisa que efectvamente el corpus se separa en párrafos y sean los párrafos correctos del texto. Tambien verifica que no se pase ningun párrafo vacio o sin palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = \"datos/quijote.txt\"\n",
    "with open(archivo, 'r', encoding='utf8') as fp:\n",
    "    corpus = fp.read()\n",
    "\n",
    "for (i, parrafo) in enumerate(texto_a_parrafo(corpus)):\n",
    "    print(\"Parrafo {}:\\n\\n{}\\n\\n\".format(i, parrafo))\n",
    "    if i == 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora revisa que cada uno de los métodos de tokenización de los párrafos realice la operación esperada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, parrafo) in enumerate(texto_a_parrafo(corpus)):\n",
    "    tokenizado = ' | '.join(parrafo_a_tokens_1(parrafo))\n",
    "    print(\"Parrafo {}:\\n\\n{}\\n\\n\".format(i, tokenizado))\n",
    "    if i == 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, parrafo) in enumerate(texto_a_parrafo(corpus)):\n",
    "    tokenizado = ' | '.join(parrafo_a_tokens_2(parrafo))\n",
    "    print(\"Parrafo {}:\\n\\n{}\\n\\n\".format(i, tokenizado))\n",
    "    if i == 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, parrafo) in enumerate(texto_a_parrafo(corpus)):\n",
    "    tokenizado = ' | '.join(parrafo_a_tokens_3(parrafo))\n",
    "    print(\"Parrafo {}:\\n\\n{}\\n\\n\".format(i, tokenizado))\n",
    "    if i == 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construyendo un modelo de $n$-gramas\n",
    "\n",
    "Ahora vamos a contruir un modelo de n-grama de manera similar a como se construyó el modelo de trigramas. Para este ejercicio nos vamos a concentrar en las ideas básicas de los modelos de lenguaje, por lo que no vamos a tomar en cuenta, ni el suavizado, ni las palabras fuera de vocabulario, ya que la complejidad extra en el código no implica una mejor comprensión de las ideas básicas.\n",
    "\n",
    "Recuerda que en este caso el modelo simplemente es la probabilidad de encontrar un $n$-grama dado, conociendo el $(n-1)$-grama formado por los primeros caracteres de dicho $n$-grama. Para desarrollar esto, vamos a hacer uso de\n",
    "la función `nltk.ngrams` (además de `collections.Counter` y `collections.defaultdict` por supuesto.\n",
    "\n",
    "Para este modelo recuerda de insertar el símbolo de inicio `<s>` y el de término `</s>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_ngrama(corpus, n=3, tokenizador=parrafo_a_tokens_1):\n",
    "    \"\"\"\n",
    "    Genera un modelo linguístico de n-gramas\n",
    "    \n",
    "    :param corpus: Una cadena de texto con el corpus completo\n",
    "    :param n: Int > 2, el grado del n-grama del modelo (default3)\n",
    "    :tokenizador: Un generador de tokens a partir de un párrafo, defaul parrafo_a_tokens_1\n",
    "    \n",
    "    :return: un modelo, el cual es un `defaultdic` tal que modelo[ngrama] es equivalente \n",
    "             a la probabilidad P(w_n|w_{n-1}, \\ldos, w_1)\n",
    "    \n",
    "    \"\"\"\n",
    "    n_gramas = (ESCRIBE AQUÍ EL GENERADOR DE N-GRAMAS)\n",
    "    \n",
    "    n_menos_uno_gramas = (ESCRIBE AQUÍ EL GENERADOR DE (N-1)-GRAMAS)\n",
    "    \n",
    "    n_cuentas = COMPLETA EL CONTADOR DE N-GRAMAS\n",
    "    \n",
    "    n_menos_uno_cuentas = ESCRIBE AQUI EL CONTADOR DE (N-1)-GRAMAS\n",
    "    \n",
    "    modelo = defaultdict(lambda: 0)\n",
    "    for ngrama in n_gramas:\n",
    "        modelo[ngrama] = COMPLETA EL CALCULO DE P(n-grama | (n-1)-grama)\n",
    "    \n",
    "    return modelo\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Responde a las siguientes preguntas:(en esta misma celda)**\n",
    "\n",
    "1. ¿Cuales son los 3 5-gramas que más se repiten para el tokenizador `parrafo_a_tokens_1`?\n",
    "2. ¿Cuales son los 3 5-gramas que más se repiten para el tokenizador `parrafo_a_tokens_2`?\n",
    "3. ¿Cuales son los 3 5-gramas que más se repiten para el tokenizador `parrafo_a_tokens_3`?\n",
    "4. ¿Cual es el 4-grama (w1, w2, w3, w4) por el cual el valor de P(w4| w3, w2, w1) es mayor (usando `parrafo_a_tokens_2`)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANEXA LOS PROGRAMAS NECESARIOS PARA CONTESTAR LAS PREGUNTAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generando párrafos de palabras\n",
    "\n",
    "Ahora vamos a hacer una función que nos permita generar párrafos a partir de un modelo. La función es muy similar a la que desarrollamos en el curso, con el detalle que esta debe funcionar para $n$-gramas de orden arbitrario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generador_parrafo(modelo):\n",
    "    \"\"\"\n",
    "    genera un parrafo a partir de un modelo de n-grama\n",
    "    \n",
    "    :param modelo: Un modelo obtenido a partir de la función modelo_ngrama\n",
    "    \n",
    "    :return: Una cadena de caracteres con una sentencia.\n",
    "    \n",
    "    \"\"\"\n",
    "    n = len(list(modelo.keys())[0])  #  n, tamaño del n-grama\n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "    # INSERTAR AQUI EL CÖDIO\n",
    "    # (revisa las libretas del curso por si puedes reusar código y adaptarlo)\n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "    return parrafo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora lo vamos a probar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo3 = modelo_ngrama(corpus, n=3, tokenizador=parrafo_a_tokens_2)\n",
    "modelo4 = modelo_ngrama(corpus, n=4, tokenizador=parrafo_a_tokens_2)\n",
    "modelo5 = modelo_ngrama(corpus, n=5, tokenizador=parrafo_a_tokens_2)\n",
    "modelo6 = modelo_ngrama(corpus, n=6, tokenizador=parrafo_a_tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parrafo_3 = generador_parrafo(modelo3)\n",
    "print(20*'=' + '\\n' + parrafo_3 + '\\n')\n",
    "\n",
    "parrafo_4 = generador_parrafo(modelo4)\n",
    "print(20*'=' + '\\n' + parrafo_4 + '\\n')\n",
    "\n",
    "parrafo_5 = generador_parrafo(modelo5)\n",
    "print(20*'=' + '\\n' + parrafo_5 + '\\n')\n",
    "\n",
    "parrafo_6 = generador_parrafo(modelo6)\n",
    "print(20*'=' + '\\n' + parrafo_6 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repites varias veces esta última celda y compara los párrafos que encuentras. \n",
    "\n",
    "**Describe en esta celda en un párrafo pequeño que es lo que encuentras.**\n",
    "\n",
    "(_ingresa aquí tu párrafo_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perplejidad\n",
    "\n",
    "Una cosa es la apreciación subjetiva y otra es el cálculo de la perplejidad de un texto respecto a un modelo dado. Vamos a calcular la perplejidad de un texto respecto a diferentes modelo.\n",
    "\n",
    "**Desarrolla la función de perplejidad para un párrafo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplejidad(parrafo, modelo):\n",
    "    \"\"\"\n",
    "    Calcula la perplejidad de un modelo frente a un parrafo\n",
    "    \n",
    "    :param parrafo: Una cadena de caracteres, la cual vamos a asumir que \n",
    "                    es un parrafo (sin saltos de linea ni texto vacio) \n",
    "    :param modelo: Un modelo obtenido a partir de la función modelo_ngrama\n",
    "    \n",
    "    :return: Un flotante con el valor de la perplejidad (None si la perplejidad es infinita)\n",
    "    \"\"\"\n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "    # INSERTAR AQUI EL CÖDIO\n",
    "    # (revisa las libretas del curso por si puedes reusar código y adaptarlo)\n",
    "    #------------------------------------------------------------------------\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a calcular la perplejidad de `parrafo_6` con los 4 modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Para el párrafo {} la perplejidad es:\".format(parrafo_6))\n",
    "print(\"Para el modelo3: {}\".format(perplejidad(parrafo6, modelo3)))\n",
    "print(\"Para el modelo4: {}\".format(perplejidad(parrafo6, modelo4)))\n",
    "print(\"Para el modelo5: {}\".format(perplejidad(parrafo6, modelo5)))\n",
    "print(\"Para el modelo6: {}\".format(perplejidad(parrafo6, modelo6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Que encontraste? ¿Que conclusiones sacas? ¿Ocurrió algo que no esperabas?\n",
    "\n",
    "**Escrbe aquí tus conclusiones**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por útimo vamos a generar un modelo con el corpus completo y vamos a calcular la perplejidad del modelo con el mismo corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genera_evalua_modelo(corpus, n=3, tokenizador=parrafo_a_tokens_1):\n",
    "    \"\"\"\n",
    "    Genera y evalúa la perplejidad de un modelo con el mismo corpus con el que se generó\n",
    "    \n",
    "    :param corpus: Una cadena de texto con el corpus completo\n",
    "    :param n: Int > 2, el grado del n-grama del modelo (default3)\n",
    "    :tokenizador: Un generador de tokens a partir de un párrafo, defaul parrafo_a_tokens_1\n",
    "\n",
    "    :return la perplejidad promedio de los párrafos\n",
    "    \n",
    "    \"\"\"\n",
    "    modelo = modelo_ngrama(corpus, n=n, tokenizador=tokenizador)\n",
    "    perp_acc, m = 0.0, 0\n",
    "    for parrafo in texto_a_parrafo(corpus):\n",
    "        perp_acc += perplejidad(parrafo, modelo)\n",
    "        m += 1\n",
    "    return perp_acc / m\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "tokenizador = parrafo_a_tokens_1\n",
    "perp_media = genera_evalua_modelo(corpus, n, tokenizador)\n",
    "print(\"Para el modelo con {}-grama, la parplejidad promedio es\".format(n, perp_media))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Completa la tabla siguiente**\n",
    "\n",
    "| Tokenizador        |   n   | Perplejidad media |\n",
    "| :----------------  |:-----:| :---------------- | \n",
    "| parrafo_a_tokens_1 | 3     |                   |\n",
    "| parrafo_a_tokens_1 | 4     |                   |\n",
    "| parrafo_a_tokens_1 | 5     |                   |\n",
    "| parrafo_a_tokens_2 | 3     |                   |\n",
    "| parrafo_a_tokens_2 | 4     |                   |\n",
    "| parrafo_a_tokens_2 | 5     |                   |\n",
    "| parrafo_a_tokens_3 | 3     |                   |\n",
    "| parrafo_a_tokens_3 | 4     |                   |\n",
    "| parrafo_a_tokens_3 | 5     |                   |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
