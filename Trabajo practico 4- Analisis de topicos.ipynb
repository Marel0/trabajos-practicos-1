{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src=\"imagenes/cabecera.png\" width=\"900\" align=\"center\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo práctico 4: Análisis de tópicos\n",
    "\n",
    "## Curso Procesamiento de Lenguaje Natural \n",
    "\n",
    "### Maestría en Tecnologías de la información\n",
    "\n",
    "\n",
    "\n",
    "**Trabajo práctico porpuesto por:** Julio Waissman Vilanova (julio.waissman@unison.mx)\n",
    "\n",
    "**Desarrollado por:** _Poner tu nombre y correo electrónico aquí_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta libreta vamos a experimentar con el análisis de tópicos a partir de un corpus en español muy interesante para realizar análisis de tópicos: Una colección de sonetos del siglo de oro español. El córpus incluye mucha información adicional (metadatos) como título, autor, y sobre todo la notación métrica del soneto. Sin embargo para este trabajo no vamos a considerar ninguna de esta información adicional.\n",
    "\n",
    "El corpus se encuentra [en este proyecto de github](https://github.com/bncolorado/CorpusSonetosSigloDeOro) y para su uso académico lo debemos citar correctamente en la referencia:\n",
    "\n",
    "> Navarro-Colorado, Borja; Ribes Lafoz, María, and Sánchez, Noelia (2015) \"Metrical annotation of a large corpus of Spanish sonnets: representation, scansion and evaluation\" 10th edition of the Language Resources and Evaluation Conference 2016 Portorož, Slovenia.[PDF](http://www.dlsi.ua.es/%7Eborja/navarro2016_MetricalPatternsBank.pdf)\n",
    "\n",
    "Lo que vamos a hacer es un análisis similar al que se realizo en [este artículo recientemente publicado](https://www.frontiersin.org/articles/10.3389/fdigh.2018.00015/full). Toma un tiempo leyendo el artículo para ver que tipo de trabajos que se publican en el área. Igualmente, este trabajo sirve para poner en prespectiva todo lo que faltaría hacer a un trabajo como el que hacemos para que sea *publicable*.\n",
    "\n",
    "## 1. Obtención y tratamiento del *corpus*\n",
    "\n",
    "Para obtener el *corpus* es necesario clonar el proyecto original. Desde el punto de mentaje (esto es, la carpeta `curso-pln` centro del contenedor (aunque tambien puede ser por fuera), se usa el comando:\n",
    "\n",
    "```\n",
    "git clone https://github.com/bncolorado/CorpusSonetosSigloDeOro.git\n",
    "```\n",
    "\n",
    "Una vez clonado, es necesario extraer la información de los archivos `xml`. La generación del *corpus* sin normalizar ya la dejo desarollada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as et # Para manejar buffers con estructura xml\n",
    "import os\n",
    "\n",
    "archivos_path = \"/root/curso-pln/CorpusSonetosSigloDeOro/\"\n",
    "\n",
    "def lee_soneto(archivo):\n",
    "    soneto = \"\"\n",
    "    arbol = et.parse(archivo)\n",
    "    raiz = arbol.getroot()\n",
    "    for poema in raiz.find('{http://www.tei-c.org/ns/1.0}text'):\n",
    "        if poema.tag == '{http://www.tei-c.org/ns/1.0}body':\n",
    "            for parrafo in poema:\n",
    "                if parrafo.tag == '{http://www.tei-c.org/ns/1.0}lg':\n",
    "                    for linea in parrafo:\n",
    "                        soneto += (linea.text + '\\n')\n",
    "                    soneto += '\\n'\n",
    "    return soneto\n",
    "\n",
    "corpus_sonetos = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(archivos_path):\n",
    "    if not dirnames:\n",
    "        for filename in filenames:\n",
    "            if filename[-4:] == '.xml':\n",
    "                corpus_sonetos.append(lee_soneto(dirpath + '/' + filename))\n",
    "            \n",
    "len(corpus_sonetos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecuta varias veces la celda de abajo para ver en forma aleatoria si los sonetos se descargaron correcatmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "print(corpus_sonetos[random.randint(0, 5077)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y por último los guardamos en un archivo `pickle` para no tener que recalcular cada vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"datos/sonetos.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(corpus_sonetos, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normalización de texto\n",
    "\n",
    "Ahora es necesario normalizar el texto con fin de utilizarlo en modelado de tópicos.\n",
    "\n",
    "El texto lo vamos a tratar con al menos los siguientes requisitos:\n",
    "\n",
    "1. Utilizar todas las palabras en minúsculas\n",
    "1. Eliminar los signos de puntuación\n",
    "2. Eliminar palabras vacias\n",
    "3. Eliminar las palabras que no aportan significados en la poesía. Dejar las palabras que sean:\n",
    "    1. pronombres personales,\n",
    "    2. sustantivos,\n",
    "    3. adjetivos, y/o\n",
    "    4. vebos.\n",
    "    \n",
    "Realiza esta normalización utilizando el módulo *spacy* con el modelo `es_core_news_sm`, pero sientete con la libertad de cargar el modelo \n",
    "`es_core_news_md` que propone *spacy*. Las instucciones para instalarlo se encuentran [aquí](https://spacy.io/models/es#section-es_core_news_md). Guarda tu corpus normalizado en la variable (lista de listas) `corpus_tratado`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERTA AQUÏ TU CÓDIGO\n",
    "# USA CUANTAS CELDAS CONSIDERES NECESARIO\n",
    "\n",
    "# GUARDA EL CORPUS TRATADO EN corpus_tratado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y utiliza esta celda para probar si se normalizaron vien los sontetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = random.randint(0, 5077)\n",
    "print(i)\n",
    "print(corpus_sonetos[i])\n",
    "print(\"********************\")\n",
    "print(corpus_tratado[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelado de tópicos con LDA\n",
    "\n",
    "Ahora, ya con el corpus tratado, desarrolla tu modelo LDA en *gensim*, siguiendo los siguientes pasos:\n",
    "\n",
    "1. Selecciona el número de tópicos a modelar (por default 20)\n",
    "2. Crea un diccionario con el corpus tratado, donde elimines a todas las\n",
    "   palabras que no aparezcan en al menos `min_df` documentos, y las palabras que \n",
    "   aparezcan el más del `max_df` porciento de documentos (por default `min_df = 5`\n",
    "   y `max_df = 0.3`).\n",
    "3. Genera un corpus listo para su uso, aplicando el metodo de bolsa de palabras.\n",
    "4. Aplica el método para modelar con LDA. Establece el número de iteraciones para\n",
    "   el método de estimación así como el numero de pasos que realiza el algoritmo\n",
    "   de optimización iterativa (por default 100 y 5, respectivamente).\n",
    "   \n",
    "Guarda el modelo en la variable `modelo_lda`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERTA AQUÍ TU CÓDIGO\n",
    "# USA CUANTAS CELDAS CONSIDERES NECESARIO\n",
    "\n",
    "# GUARDA EL MODELO EN modelo_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora veamos los modelos como se definen por sus primeras 5 palabras clave "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "for (i, topico) in modelo_lda.print_topics(num_topics=n_topicos, num_words=5):\n",
    "    print(10 * \"-\" + \"topico {}\".format(i) + 20 * \"-\")\n",
    "    print(topico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejor aún, utiliza *pyLDAvis* para visualizar y analizar los tópicos desarrollados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERTA AQUÏ TU CÓDIGO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Responde a las preguntas siguientes\n",
    "\n",
    "Cada pregunta implica experimentación y reflexión, por favor extiende lo que consideres necesario tus explicaciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ¿Cual es el número de tópicos que consideras te ofrece una mejor separación entre ellos, y mejor significado (dejando al resto de los parámetros en valores por default)? ¿Porqué? ¿Algunos tópicos son redundantes entre si? ¿Podrías asignarle un nombre a cada tópico?\n",
    "\n",
    "_ingresa aquí tu respuesta_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ¿Que pasa si eliminas los pronombres personales en la normalización del texto? ¿Y los verbos?¿Que pasa si lematizas? ¿Hay alguna modificación a la normalización de texto que implique una mejor distribución de tópicos (con el resto de los valores por default y el número de tópicos que seleccionaste en la pregunta 1)?\n",
    "\n",
    "_ingresa aquí tu respuesta_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ¿Que pasa si reduces el número de iteraciones en la inferencia a 50? ¿Que pasa si la aumentas a 200? ¿Y si reduces el numero de iteraciones del algoritmo de optimización a solo 1? ¿Si lo aumentas a 100 (ten cuidado, puede tardar mucho)? ¿Consideras que estos parámetros tienen mucha influencia en el resultado del método de LDA?\n",
    "\n",
    "_ingresa aquí tu respuesta_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ¿Que pasa si aceptas todas las palabras que aparezcan en menos del 80% de los documentos? ¿Que pasaría si limitas el vocabulario a las palabras que solamente aparezcan en el $\\frac{1.5}{\\text(numero\\ de\\ tópicos)}$ por ciento de los documentos? ¿Cual es el equilibrio que consideras que es el más acertado? \n",
    "\n",
    "_ingresa aquí tu respuesta_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Combina todo y obten un modelado de tópicos que consideres es interesante. Escribe tus conclusiones y los tópicos que pudiste extraer del corpus bajo análisis, así como el título que pondrías a estos (o algunos de estos).\n",
    "\n",
    "Si tienes más de un modelo que te parece interesante, puedes utilizar los métodos de medición de coherencia de los tópicos para tomar una desición.\n",
    "\n",
    "_ingresa aquí tu respuesta_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
